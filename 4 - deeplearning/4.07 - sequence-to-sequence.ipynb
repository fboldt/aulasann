{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !wget http://storage.googleapis.com/download.tensorflow.org/data/spa-eng.zip\n",
    "# !unzip -q spa-eng.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_file = \"spa-eng/spa.txt\"\n",
    "with open(text_file) as f:\n",
    "    lines = f.read().split(\"\\n\")[:-1]\n",
    "text_pairs = []\n",
    "for line in lines:\n",
    "    english, spanish = line.split(\"\\t\")\n",
    "    spanish = \"[start] \" + spanish + \" [end]\"\n",
    "    text_pairs.append((english, spanish))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Stop saying that!', '[start] ¡Para de decir eso! [end]')\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "print(random.choice(text_pairs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "random.shuffle(text_pairs)\n",
    "num_val_samples = int(0.15 * len(text_pairs))\n",
    "num_train_samples = len(text_pairs) - 2 * num_val_samples\n",
    "train_pairs = text_pairs[:num_train_samples]\n",
    "val_pairs = text_pairs[num_train_samples:num_train_samples + num_val_samples]\n",
    "test_pairs = text_pairs[num_train_samples + num_val_samples:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-12 18:53:36.636251: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-06-12 18:53:37.224300: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "2023-06-12 18:53:37.700618: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-06-12 18:53:37.716941: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-06-12 18:53:37.717275: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-06-12 18:53:37.718267: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-06-12 18:53:37.718568: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-06-12 18:53:37.718846: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-06-12 18:53:38.173439: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-06-12 18:53:38.174111: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-06-12 18:53:38.174531: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-06-12 18:53:38.174881: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1635] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 10082 MB memory:  -> device: 0, name: NVIDIA TITAN V, pci bus id: 0000:01:00.0, compute capability: 7.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import string\n",
    "import re\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "strip_chars = string.punctuation + \"¿\"\n",
    "strip_chars = strip_chars.replace(\"[\", \"\")\n",
    "strip_chars = strip_chars.replace(\"]\", \"\")\n",
    "\n",
    "def custom_standardization(input_string):\n",
    "    lowercase = tf.strings.lower(input_string)\n",
    "    return tf.strings.regex_replace(\n",
    "        lowercase, f\"[{re.escape(strip_chars)}]\", \"\")\n",
    "\n",
    "vocab_size = 15000\n",
    "sequence_length = 20\n",
    "\n",
    "source_vectorization = layers.TextVectorization(\n",
    "    max_tokens=vocab_size,\n",
    "    output_mode=\"int\",\n",
    "    output_sequence_length=sequence_length,\n",
    ")\n",
    "target_vectorization = layers.TextVectorization(\n",
    "    max_tokens=vocab_size,\n",
    "    output_mode=\"int\",\n",
    "    output_sequence_length=sequence_length + 1,\n",
    "    standardize=custom_standardization,\n",
    ")\n",
    "train_english_texts = [pair[0] for pair in train_pairs]\n",
    "train_spanish_texts = [pair[1] for pair in train_pairs]\n",
    "source_vectorization.adapt(train_english_texts)\n",
    "target_vectorization.adapt(train_spanish_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "\n",
    "def format_dataset(eng, spa):\n",
    "    eng = source_vectorization(eng)\n",
    "    spa = target_vectorization(spa)\n",
    "    return ({\n",
    "        \"english\": eng,\n",
    "        \"spanish\": spa[:, :-1],\n",
    "    }, spa[:, 1:])\n",
    "\n",
    "def make_dataset(pairs):\n",
    "    eng_texts, spa_texts = zip(*pairs)\n",
    "    eng_texts = list(eng_texts)\n",
    "    spa_texts = list(spa_texts)\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((eng_texts, spa_texts))\n",
    "    dataset = dataset.batch(batch_size)\n",
    "    dataset = dataset.map(format_dataset, num_parallel_calls=4)\n",
    "    return dataset.shuffle(2048).prefetch(16).cache()\n",
    "\n",
    "train_ds = make_dataset(train_pairs)\n",
    "val_ds = make_dataset(val_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs['english'].shape: (64, 20)\n",
      "inputs['spanish'].shape: (64, 20)\n",
      "targets.shape: (64, 20)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-12 18:54:04.329418: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_7' with dtype int64\n",
      "\t [[{{node Placeholder/_7}}]]\n",
      "2023-06-12 18:54:04.329826: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_13' with dtype int64\n",
      "\t [[{{node Placeholder/_13}}]]\n",
      "2023-06-12 18:54:04.446218: W tensorflow/core/kernels/data/cache_dataset_ops.cc:856] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    }
   ],
   "source": [
    "for inputs, targets in train_ds.take(1):\n",
    "    print(f\"inputs['english'].shape: {inputs['english'].shape}\")\n",
    "    print(f\"inputs['spanish'].shape: {inputs['spanish'].shape}\")\n",
    "    print(f\"targets.shape: {targets.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "embed_dim = 256\n",
    "latent_dim = 1024\n",
    "\n",
    "source = keras.Input(shape=(None,), dtype=\"int64\", name=\"english\")\n",
    "x = layers.Embedding(vocab_size, embed_dim, mask_zero=True)(source)\n",
    "encoded_source = layers.Bidirectional(\n",
    "    layers.GRU(latent_dim), merge_mode=\"sum\")(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "past_target = keras.Input(shape=(None,), dtype=\"int64\", name=\"spanish\")\n",
    "x = layers.Embedding(vocab_size, embed_dim, mask_zero=True)(past_target)\n",
    "decoder_gru = layers.GRU(latent_dim, return_sequences=True)\n",
    "x = decoder_gru(x, initial_state=encoded_source)\n",
    "x = layers.Dropout(0.5)(x)\n",
    "target_next_step = layers.Dense(vocab_size, activation=\"softmax\")(x)\n",
    "seq2seq_rnn = keras.Model([source, past_target], target_next_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-12 18:54:07.337360: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_13' with dtype int64\n",
      "\t [[{{node Placeholder/_13}}]]\n",
      "2023-06-12 18:54:07.337728: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_13' with dtype int64\n",
      "\t [[{{node Placeholder/_13}}]]\n",
      "2023-06-12 18:54:09.620255: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/ReverseV2_grad/ReverseV2/ReverseV2/axis' with dtype int32 and shape [1]\n",
      "\t [[{{node gradients/ReverseV2_grad/ReverseV2/ReverseV2/axis}}]]\n",
      "2023-06-12 18:54:11.931070: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/ReverseV2_grad/ReverseV2/ReverseV2/axis' with dtype int32 and shape [1]\n",
      "\t [[{{node gradients/ReverseV2_grad/ReverseV2/ReverseV2/axis}}]]\n",
      "2023-06-12 18:54:12.884532: W tensorflow/core/common_runtime/type_inference.cc:339] Type inference failed. This indicates an invalid graph that escaped type checking. Error message: INVALID_ARGUMENT: expected compatible input types, but input 1:\n",
      "type_id: TFT_OPTIONAL\n",
      "args {\n",
      "  type_id: TFT_PRODUCT\n",
      "  args {\n",
      "    type_id: TFT_TENSOR\n",
      "    args {\n",
      "      type_id: TFT_INT32\n",
      "    }\n",
      "  }\n",
      "}\n",
      " is neither a subtype nor a supertype of the combined inputs preceding it:\n",
      "type_id: TFT_OPTIONAL\n",
      "args {\n",
      "  type_id: TFT_PRODUCT\n",
      "  args {\n",
      "    type_id: TFT_TENSOR\n",
      "    args {\n",
      "      type_id: TFT_FLOAT\n",
      "    }\n",
      "  }\n",
      "}\n",
      "\n",
      "\twhile inferring type of node 'cond_41/output/_22'\n",
      "2023-06-12 18:54:13.404572: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:424] Loaded cuDNN version 8600\n",
      "2023-06-12 18:54:13.689487: I tensorflow/compiler/xla/service/service.cc:169] XLA service 0x7f1c4882a640 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2023-06-12 18:54:13.689508: I tensorflow/compiler/xla/service/service.cc:177]   StreamExecutor device (0): NVIDIA TITAN V, Compute Capability 7.0\n",
      "2023-06-12 18:54:13.692453: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2023-06-12 18:54:13.734944: I tensorflow/tsl/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2023-06-12 18:54:13.779303: I ./tensorflow/compiler/jit/device_compiler.h:180] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1302/1302 [==============================] - ETA: 0s - loss: 4.6875 - accuracy: 0.3185"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-12 18:55:31.895249: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_9' with dtype int64\n",
      "\t [[{{node Placeholder/_9}}]]\n",
      "2023-06-12 18:55:31.896215: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_12' with dtype string\n",
      "\t [[{{node Placeholder/_12}}]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1302/1302 [==============================] - 94s 67ms/step - loss: 4.6875 - accuracy: 0.3185 - val_loss: 3.8628 - val_accuracy: 0.3934\n",
      "Epoch 2/15\n",
      "1302/1302 [==============================] - 78s 60ms/step - loss: 3.7123 - accuracy: 0.4167 - val_loss: 3.2193 - val_accuracy: 0.4745\n",
      "Epoch 3/15\n",
      "1302/1302 [==============================] - 79s 61ms/step - loss: 3.2007 - accuracy: 0.4741 - val_loss: 2.8456 - val_accuracy: 0.5227\n",
      "Epoch 4/15\n",
      "1302/1302 [==============================] - 79s 61ms/step - loss: 2.8455 - accuracy: 0.5151 - val_loss: 2.6072 - val_accuracy: 0.5552\n",
      "Epoch 5/15\n",
      "1302/1302 [==============================] - 79s 61ms/step - loss: 2.5693 - accuracy: 0.5486 - val_loss: 2.4138 - val_accuracy: 0.5834\n",
      "Epoch 6/15\n",
      "1302/1302 [==============================] - 79s 61ms/step - loss: 2.3454 - accuracy: 0.5766 - val_loss: 2.2858 - val_accuracy: 0.6015\n",
      "Epoch 7/15\n",
      "1302/1302 [==============================] - 79s 61ms/step - loss: 2.1556 - accuracy: 0.6016 - val_loss: 2.1886 - val_accuracy: 0.6169\n",
      "Epoch 8/15\n",
      "1302/1302 [==============================] - 79s 61ms/step - loss: 1.9995 - accuracy: 0.6222 - val_loss: 2.1233 - val_accuracy: 0.6288\n",
      "Epoch 9/15\n",
      "1302/1302 [==============================] - 79s 61ms/step - loss: 1.8603 - accuracy: 0.6410 - val_loss: 2.0592 - val_accuracy: 0.6376\n",
      "Epoch 10/15\n",
      "1302/1302 [==============================] - 79s 61ms/step - loss: 1.7412 - accuracy: 0.6576 - val_loss: 2.0160 - val_accuracy: 0.6461\n",
      "Epoch 11/15\n",
      "1302/1302 [==============================] - 79s 61ms/step - loss: 1.6344 - accuracy: 0.6726 - val_loss: 1.9855 - val_accuracy: 0.6512\n",
      "Epoch 12/15\n",
      "1302/1302 [==============================] - 79s 61ms/step - loss: 1.5418 - accuracy: 0.6855 - val_loss: 1.9576 - val_accuracy: 0.6547\n",
      "Epoch 13/15\n",
      "1302/1302 [==============================] - 79s 61ms/step - loss: 1.4589 - accuracy: 0.6975 - val_loss: 1.9403 - val_accuracy: 0.6586\n",
      "Epoch 14/15\n",
      "1302/1302 [==============================] - 79s 61ms/step - loss: 1.3883 - accuracy: 0.7085 - val_loss: 1.9252 - val_accuracy: 0.6612\n",
      "Epoch 15/15\n",
      "1302/1302 [==============================] - 79s 61ms/step - loss: 1.3263 - accuracy: 0.7181 - val_loss: 1.9130 - val_accuracy: 0.6642\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f1d2c6d1f90>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq2seq_rnn.compile(\n",
    "    optimizer=\"rmsprop\",\n",
    "    loss=\"sparse_categorical_crossentropy\",\n",
    "    metrics=[\"accuracy\"])\n",
    "seq2seq_rnn.fit(train_ds, epochs=15, validation_data=val_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-\n",
      "Are they coming here tomorrow?\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "[start] ellos vendrá aquí mañana [end]\n",
      "-\n",
      "Tom's reached a point in his life where he has to make some pretty important decisions.\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "[start] tom se preguntaba por qué mary se ha convertido en un restaurante de vida pero tiene problemas para ser difícil\n",
      "-\n",
      "Tom looked deeply into Mary's eyes and smiled.\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "[start] tom miró miró en la ojos y los ojos a mary [end]\n",
      "-\n",
      "I'm going to make you an offer that you can't refuse.\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "[start] voy a darte una [UNK] que no te puedes [UNK] [end]\n",
      "-\n",
      "Tom doesn't understand what Mary is trying to say.\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "[start] tom no entiende lo que está haciendo mary [end]\n",
      "-\n",
      "I am hers and she is mine.\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "[start] soy la y y es mi primo [end]\n",
      "-\n",
      "I thought it was just a metaphor.\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "[start] pensé que era un sueño [end]\n",
      "-\n",
      "I thought that might be the case.\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "[start] pensé que podría ser el jefe [end]\n",
      "-\n",
      "This is her handbag.\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "[start] este es su maleta [end]\n",
      "-\n",
      "You must not speak so loudly here.\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "[start] no debes hablar tan aquí aquí [end]\n",
      "-\n",
      "There was no response to my question.\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "[start] no hubo respuesta a mi pregunta [end]\n",
      "-\n",
      "She has no figure.\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "[start] ella no tiene planta [end]\n",
      "-\n",
      "The campaign succeeded and he won the election.\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "[start] el [UNK] se [UNK] y [UNK] las [UNK] [end]\n",
      "-\n",
      "People often fall asleep while reading.\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "[start] la gente se puede menudo se parece a leer [end]\n",
      "-\n",
      "I've met people like that.\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "[start] me he visto a nadie [end]\n",
      "-\n",
      "His brother is married, but he doesn't have children.\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "[start] su hermano es mayor pero no tiene hijos [end]\n",
      "-\n",
      "My left arm is asleep.\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "[start] me duele el brazo se me ha dormido [end]\n",
      "-\n",
      "We're out of money.\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "[start] estamos en dinero [end]\n",
      "-\n",
      "He made up a story about the dog.\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "[start] Él hizo un discurso de inglés [end]\n",
      "-\n",
      "I'd like to extend my stay through Sunday.\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "[start] me gustaría reservar el aire de mi domingo [end]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "spa_vocab = target_vectorization.get_vocabulary()\n",
    "spa_index_lookup = dict(zip(range(len(spa_vocab)), spa_vocab))\n",
    "max_decoded_sentence_length = 20\n",
    "\n",
    "def decode_sequence(input_sentence):\n",
    "    tokenized_input_sentence = source_vectorization([input_sentence])\n",
    "    decoded_sentence = \"[start]\"\n",
    "    for i in range(max_decoded_sentence_length):\n",
    "        tokenized_target_sentence = target_vectorization([decoded_sentence])\n",
    "        next_token_predictions = seq2seq_rnn.predict(\n",
    "            [tokenized_input_sentence, tokenized_target_sentence])\n",
    "        sampled_token_index = np.argmax(next_token_predictions[0, i, :])\n",
    "        sampled_token = spa_index_lookup[sampled_token_index]\n",
    "        decoded_sentence += \" \" + sampled_token\n",
    "        if sampled_token == \"[end]\":\n",
    "            break\n",
    "    return decoded_sentence\n",
    "\n",
    "test_eng_texts = [pair[0] for pair in test_pairs]\n",
    "for _ in range(20):\n",
    "    input_sentence = random.choice(test_eng_texts)\n",
    "    print(\"-\")\n",
    "    print(input_sentence)\n",
    "    print(decode_sequence(input_sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "class TransformerEncoder(layers.Layer):\n",
    "    def __init__(self, embed_dim, dense_dim, num_heads, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.embed_dim = embed_dim\n",
    "        self.dense_dim = dense_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.attention = layers.MultiHeadAttention(\n",
    "            num_heads=num_heads, key_dim=embed_dim)\n",
    "        self.dense_proj = keras.Sequential(\n",
    "            [layers.Dense(dense_dim, activation=\"relu\"),\n",
    "             layers.Dense(embed_dim),]\n",
    "        )\n",
    "        self.layernorm_1 = layers.LayerNormalization()\n",
    "        self.layernorm_2 = layers.LayerNormalization()\n",
    "\n",
    "    def call(self, inputs, mask=None):\n",
    "        if mask is not None:\n",
    "            mask = mask[:, tf.newaxis, :]\n",
    "        attention_output = self.attention(\n",
    "            inputs, inputs, attention_mask=mask)\n",
    "        proj_input = self.layernorm_1(inputs + attention_output)\n",
    "        proj_output = self.dense_proj(proj_input)\n",
    "        return self.layernorm_2(proj_input + proj_output)\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({\n",
    "            \"embed_dim\": self.embed_dim,\n",
    "            \"num_heads\": self.num_heads,\n",
    "            \"dense_dim\": self.dense_dim,\n",
    "        })\n",
    "        return config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerDecoder(layers.Layer):\n",
    "    def __init__(self, embed_dim, dense_dim, num_heads, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.embed_dim = embed_dim\n",
    "        self.dense_dim = dense_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.attention_1 = layers.MultiHeadAttention(\n",
    "            num_heads=num_heads, key_dim=embed_dim)\n",
    "        self.attention_2 = layers.MultiHeadAttention(\n",
    "            num_heads=num_heads, key_dim=embed_dim)\n",
    "        self.dense_proj = keras.Sequential(\n",
    "            [layers.Dense(dense_dim, activation=\"relu\"),\n",
    "             layers.Dense(embed_dim),]\n",
    "        )\n",
    "        self.layernorm_1 = layers.LayerNormalization()\n",
    "        self.layernorm_2 = layers.LayerNormalization()\n",
    "        self.layernorm_3 = layers.LayerNormalization()\n",
    "        self.supports_masking = True\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({\n",
    "            \"embed_dim\": self.embed_dim,\n",
    "            \"num_heads\": self.num_heads,\n",
    "            \"dense_dim\": self.dense_dim,\n",
    "        })\n",
    "        return config\n",
    "\n",
    "    def get_causal_attention_mask(self, inputs):\n",
    "        input_shape = tf.shape(inputs)\n",
    "        batch_size, sequence_length = input_shape[0], input_shape[1]\n",
    "        i = tf.range(sequence_length)[:, tf.newaxis]\n",
    "        j = tf.range(sequence_length)\n",
    "        mask = tf.cast(i >= j, dtype=\"int32\")\n",
    "        mask = tf.reshape(mask, (1, input_shape[1], input_shape[1]))\n",
    "        mult = tf.concat(\n",
    "            [tf.expand_dims(batch_size, -1),\n",
    "             tf.constant([1, 1], dtype=tf.int32)], axis=0)\n",
    "        return tf.tile(mask, mult)\n",
    "\n",
    "    def call(self, inputs, encoder_outputs, mask=None):\n",
    "        causal_mask = self.get_causal_attention_mask(inputs)\n",
    "        if mask is not None:\n",
    "            padding_mask = tf.cast(\n",
    "                mask[:, tf.newaxis, :], dtype=\"int32\")\n",
    "            padding_mask = tf.minimum(padding_mask, causal_mask)\n",
    "        else:\n",
    "            padding_mask = mask\n",
    "        attention_output_1 = self.attention_1(\n",
    "            query=inputs,\n",
    "            value=inputs,\n",
    "            key=inputs,\n",
    "            attention_mask=causal_mask)\n",
    "        attention_output_1 = self.layernorm_1(inputs + attention_output_1)\n",
    "        attention_output_2 = self.attention_2(\n",
    "            query=attention_output_1,\n",
    "            value=encoder_outputs,\n",
    "            key=encoder_outputs,\n",
    "            attention_mask=padding_mask,\n",
    "        )\n",
    "        attention_output_2 = self.layernorm_2(\n",
    "            attention_output_1 + attention_output_2)\n",
    "        proj_output = self.dense_proj(attention_output_2)\n",
    "        return self.layernorm_3(attention_output_2 + proj_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEmbedding(layers.Layer):\n",
    "    def __init__(self, sequence_length, input_dim, output_dim, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.token_embeddings = layers.Embedding(\n",
    "            input_dim=input_dim, output_dim=output_dim)\n",
    "        self.position_embeddings = layers.Embedding(\n",
    "            input_dim=sequence_length, output_dim=output_dim)\n",
    "        self.sequence_length = sequence_length\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "\n",
    "    def call(self, inputs):\n",
    "        length = tf.shape(inputs)[-1]\n",
    "        positions = tf.range(start=0, limit=length, delta=1)\n",
    "        embedded_tokens = self.token_embeddings(inputs)\n",
    "        embedded_positions = self.position_embeddings(positions)\n",
    "        return embedded_tokens + embedded_positions\n",
    "\n",
    "    def compute_mask(self, inputs, mask=None):\n",
    "        return tf.math.not_equal(inputs, 0)\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super(PositionalEmbedding, self).get_config()\n",
    "        config.update({\n",
    "            \"output_dim\": self.output_dim,\n",
    "            \"sequence_length\": self.sequence_length,\n",
    "            \"input_dim\": self.input_dim,\n",
    "        })\n",
    "        return config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_dim = 256\n",
    "dense_dim = 2048\n",
    "num_heads = 8\n",
    "\n",
    "encoder_inputs = keras.Input(shape=(None,), dtype=\"int64\", name=\"english\")\n",
    "x = PositionalEmbedding(sequence_length, vocab_size, embed_dim)(encoder_inputs)\n",
    "encoder_outputs = TransformerEncoder(embed_dim, dense_dim, num_heads)(x)\n",
    "\n",
    "decoder_inputs = keras.Input(shape=(None,), dtype=\"int64\", name=\"spanish\")\n",
    "x = PositionalEmbedding(sequence_length, vocab_size, embed_dim)(decoder_inputs)\n",
    "x = TransformerDecoder(embed_dim, dense_dim, num_heads)(x, encoder_outputs)\n",
    "x = layers.Dropout(0.5)(x)\n",
    "decoder_outputs = layers.Dense(vocab_size, activation=\"softmax\")(x)\n",
    "transformer = keras.Model([encoder_inputs, decoder_inputs], decoder_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "1302/1302 [==============================] - 73s 53ms/step - loss: 3.7943 - accuracy: 0.4399 - val_loss: 2.8619 - val_accuracy: 0.5449\n",
      "Epoch 2/30\n",
      "1302/1302 [==============================] - 63s 48ms/step - loss: 2.8522 - accuracy: 0.5494 - val_loss: 2.5013 - val_accuracy: 0.5941\n",
      "Epoch 3/30\n",
      "1302/1302 [==============================] - 63s 48ms/step - loss: 2.5594 - accuracy: 0.5926 - val_loss: 2.3910 - val_accuracy: 0.6145\n",
      "Epoch 4/30\n",
      "1302/1302 [==============================] - 63s 49ms/step - loss: 2.3962 - accuracy: 0.6191 - val_loss: 2.3050 - val_accuracy: 0.6293\n",
      "Epoch 5/30\n",
      "1302/1302 [==============================] - 63s 48ms/step - loss: 2.2906 - accuracy: 0.6376 - val_loss: 2.2951 - val_accuracy: 0.6349\n",
      "Epoch 6/30\n",
      "1302/1302 [==============================] - 63s 48ms/step - loss: 2.2192 - accuracy: 0.6517 - val_loss: 2.2866 - val_accuracy: 0.6390\n",
      "Epoch 7/30\n",
      "1302/1302 [==============================] - 63s 49ms/step - loss: 2.1632 - accuracy: 0.6622 - val_loss: 2.2961 - val_accuracy: 0.6442\n",
      "Epoch 8/30\n",
      "1302/1302 [==============================] - 63s 48ms/step - loss: 2.1005 - accuracy: 0.6745 - val_loss: 2.2681 - val_accuracy: 0.6477\n",
      "Epoch 9/30\n",
      "1302/1302 [==============================] - 63s 48ms/step - loss: 2.0418 - accuracy: 0.6855 - val_loss: 2.2331 - val_accuracy: 0.6581\n",
      "Epoch 10/30\n",
      "1302/1302 [==============================] - 63s 49ms/step - loss: 1.9939 - accuracy: 0.6945 - val_loss: 2.2305 - val_accuracy: 0.6641\n",
      "Epoch 11/30\n",
      "1302/1302 [==============================] - 63s 48ms/step - loss: 1.9563 - accuracy: 0.7011 - val_loss: 2.2218 - val_accuracy: 0.6657\n",
      "Epoch 12/30\n",
      "1302/1302 [==============================] - 63s 48ms/step - loss: 1.9211 - accuracy: 0.7074 - val_loss: 2.2424 - val_accuracy: 0.6624\n",
      "Epoch 13/30\n",
      "1302/1302 [==============================] - 63s 48ms/step - loss: 1.8960 - accuracy: 0.7120 - val_loss: 2.2334 - val_accuracy: 0.6682\n",
      "Epoch 14/30\n",
      "1302/1302 [==============================] - 63s 48ms/step - loss: 1.8706 - accuracy: 0.7172 - val_loss: 2.2679 - val_accuracy: 0.6652\n",
      "Epoch 15/30\n",
      "1302/1302 [==============================] - 63s 49ms/step - loss: 1.8477 - accuracy: 0.7213 - val_loss: 2.2646 - val_accuracy: 0.6699\n",
      "Epoch 16/30\n",
      "1302/1302 [==============================] - 62s 48ms/step - loss: 1.8258 - accuracy: 0.7257 - val_loss: 2.2749 - val_accuracy: 0.6685\n",
      "Epoch 17/30\n",
      "1302/1302 [==============================] - 63s 48ms/step - loss: 1.8084 - accuracy: 0.7284 - val_loss: 2.2799 - val_accuracy: 0.6675\n",
      "Epoch 18/30\n",
      "1302/1302 [==============================] - 63s 48ms/step - loss: 1.7923 - accuracy: 0.7313 - val_loss: 2.3281 - val_accuracy: 0.6616\n",
      "Epoch 19/30\n",
      "1302/1302 [==============================] - 62s 48ms/step - loss: 1.7769 - accuracy: 0.7340 - val_loss: 2.3125 - val_accuracy: 0.6713\n",
      "Epoch 20/30\n",
      "1302/1302 [==============================] - 63s 48ms/step - loss: 1.7597 - accuracy: 0.7371 - val_loss: 2.3102 - val_accuracy: 0.6725\n",
      "Epoch 21/30\n",
      "1302/1302 [==============================] - 63s 48ms/step - loss: 1.7464 - accuracy: 0.7399 - val_loss: 2.3075 - val_accuracy: 0.6720\n",
      "Epoch 22/30\n",
      "1302/1302 [==============================] - 63s 49ms/step - loss: 1.7336 - accuracy: 0.7425 - val_loss: 2.3304 - val_accuracy: 0.6697\n",
      "Epoch 23/30\n",
      "1302/1302 [==============================] - 63s 48ms/step - loss: 1.7161 - accuracy: 0.7459 - val_loss: 2.3419 - val_accuracy: 0.6701\n",
      "Epoch 24/30\n",
      "1302/1302 [==============================] - 63s 48ms/step - loss: 1.7053 - accuracy: 0.7477 - val_loss: 2.3332 - val_accuracy: 0.6741\n",
      "Epoch 25/30\n",
      "1302/1302 [==============================] - 63s 48ms/step - loss: 1.6901 - accuracy: 0.7499 - val_loss: 2.3776 - val_accuracy: 0.6675\n",
      "Epoch 26/30\n",
      "1302/1302 [==============================] - 62s 48ms/step - loss: 1.6747 - accuracy: 0.7527 - val_loss: 2.3873 - val_accuracy: 0.6694\n",
      "Epoch 27/30\n",
      "1302/1302 [==============================] - 63s 48ms/step - loss: 1.6625 - accuracy: 0.7550 - val_loss: 2.3735 - val_accuracy: 0.6729\n",
      "Epoch 28/30\n",
      "1302/1302 [==============================] - 63s 48ms/step - loss: 1.6544 - accuracy: 0.7564 - val_loss: 2.3916 - val_accuracy: 0.6703\n",
      "Epoch 29/30\n",
      "1302/1302 [==============================] - 63s 48ms/step - loss: 1.6403 - accuracy: 0.7587 - val_loss: 2.4073 - val_accuracy: 0.6700\n",
      "Epoch 30/30\n",
      "1302/1302 [==============================] - 62s 48ms/step - loss: 1.6272 - accuracy: 0.7610 - val_loss: 2.4104 - val_accuracy: 0.6731\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f1c69685ff0>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformer.compile(\n",
    "    optimizer=\"rmsprop\",\n",
    "    loss=\"sparse_categorical_crossentropy\",\n",
    "    metrics=[\"accuracy\"])\n",
    "transformer.fit(train_ds, epochs=30, validation_data=val_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-\n",
      "We're safe in here, aren't we?\n",
      "[start] estamos a john aquí no estamos [end]\n",
      "-\n",
      "We'll go back to Boston next week.\n",
      "[start] bueno vuelta para la próxima vez [end]\n",
      "-\n",
      "Delegates from many countries participated in the conference.\n",
      "[start] los fantasmas de los países [UNK] están [UNK] [end]\n",
      "-\n",
      "The price of eggs is going up.\n",
      "[start] el precio de huevos es [UNK] [end]\n",
      "-\n",
      "You said you loved me.\n",
      "[start] tú me dijeron que me [UNK] [end]\n",
      "-\n",
      "Here I am again.\n",
      "[start] aquí otra vez [end]\n",
      "-\n",
      "He showed interest in the plan.\n",
      "[start] Él mostró el médico en el plan [end]\n",
      "-\n",
      "I'm from Kyoto.\n",
      "[start] tengo kioto [end]\n",
      "-\n",
      "Anyone hurt?\n",
      "[start] alguien hizo daño [end]\n",
      "-\n",
      "Her children have gone to Tokyo and she misses them very much.\n",
      "[start] sus hijos han ido a tokio y los muchos niños han para que han [UNK] [end]\n",
      "-\n",
      "Where's your baby?\n",
      "[start] dónde está tu bebé [end]\n",
      "-\n",
      "I knew it would happen sooner or later.\n",
      "[start] sabía que [UNK] tarde o temprano [end]\n",
      "-\n",
      "Tom was bewildered by the change.\n",
      "[start] tom estaba [UNK] por el cambio [end]\n",
      "-\n",
      "Many wives complain about high prices.\n",
      "[start] muchos [UNK] de un país a punto [end]\n",
      "-\n",
      "Are you going to school tomorrow?\n",
      "[start] vas mañana a la escuela [end]\n",
      "-\n",
      "Don't cry.\n",
      "[start] no llores [end]\n",
      "-\n",
      "What would you advise me to do?\n",
      "[start] qué me [UNK] hacer [end]\n",
      "-\n",
      "Tom peeled the potatoes.\n",
      "[start] tom [UNK] las ostras [end]\n",
      "-\n",
      "I'm cold. May I close the window?\n",
      "[start] tengo frío puedes ir cerca de la ventana [end]\n",
      "-\n",
      "We spent the afternoon eating grapes.\n",
      "[start] nos pasó la tarde [UNK] las dos vegetales [end]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "spa_vocab = target_vectorization.get_vocabulary()\n",
    "spa_index_lookup = dict(zip(range(len(spa_vocab)), spa_vocab))\n",
    "max_decoded_sentence_length = 20\n",
    "\n",
    "def decode_sequence(input_sentence):\n",
    "    tokenized_input_sentence = source_vectorization([input_sentence])\n",
    "    decoded_sentence = \"[start]\"\n",
    "    for i in range(max_decoded_sentence_length):\n",
    "        tokenized_target_sentence = target_vectorization(\n",
    "            [decoded_sentence])[:, :-1]\n",
    "        predictions = transformer(\n",
    "            [tokenized_input_sentence, tokenized_target_sentence])\n",
    "        sampled_token_index = np.argmax(predictions[0, i, :])\n",
    "        sampled_token = spa_index_lookup[sampled_token_index]\n",
    "        decoded_sentence += \" \" + sampled_token\n",
    "        if sampled_token == \"[end]\":\n",
    "            break\n",
    "    return decoded_sentence\n",
    "\n",
    "test_eng_texts = [pair[0] for pair in test_pairs]\n",
    "for _ in range(20):\n",
    "    input_sentence = random.choice(test_eng_texts)\n",
    "    print(\"-\")\n",
    "    print(input_sentence)\n",
    "    print(decode_sequence(input_sentence))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
