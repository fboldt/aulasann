{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOHJS3pMrv3EPoZADz2O2RO",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/fboldt/aulasann/blob/main/aula14a_gera%C3%A7%C3%A3o_de_texto.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4ovJGCvkH0MW",
        "outputId": "443b3a7d-8a43-4fd2-80da-cdf184c8306f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100 80.2M  100 80.2M    0     0  10.7M      0  0:00:07  0:00:07 --:--:-- 13.4M\n"
          ]
        }
      ],
      "source": [
        "!curl -O https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
        "!tar -xf aclImdb_v1.tar.gz"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "dataset = keras.utils.text_dataset_from_directory(\n",
        "    directory='aclImdb',\n",
        "    batch_size=256,\n",
        "    label_mode=None,\n",
        ")\n",
        "dataset = dataset.map(lambda x: tf.strings.regex_replace(x, \"<br />\", \" \"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XAOwMbvVKtjW",
        "outputId": "ba33e8af-1b9d-47b1-e9ad-00d3ff5a4660"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 100006 files belonging to 1 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.layers import TextVectorization\n",
        "\n",
        "sequence_length = 100\n",
        "vocab_size = 15000\n",
        "text_vectorization = TextVectorization(\n",
        "    max_tokens = vocab_size,\n",
        "    output_mode = \"int\",\n",
        "    output_sequence_length = sequence_length,\n",
        ")\n",
        "text_vectorization.adapt(dataset)"
      ],
      "metadata": {
        "id": "q0hj0u_pLnzE"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def prepare_lm_dataset(text_batch):\n",
        "  vectorized_sequences = text_vectorization(text_batch)\n",
        "  x = vectorized_sequences[:, :-1]\n",
        "  y = vectorized_sequences[:, 1:]\n",
        "  return x, y\n",
        "\n",
        "lm_dataset = dataset.map(prepare_lm_dataset)"
      ],
      "metadata": {
        "id": "1PCdjMeQMN2H"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "class PositionalEmbedding(layers.Layer):\n",
        "  def __init__(self, sequence_length, input_dim, output_dim, **kwargs):\n",
        "    super().__init__(**kwargs)\n",
        "    self.token_embeddings = layers.Embedding(\n",
        "        input_dim=input_dim, output_dim=output_dim\n",
        "    )\n",
        "    self.position_embeddings = layers.Embedding(\n",
        "        input_dim=sequence_length, output_dim=output_dim\n",
        "    )\n",
        "    self.sequence_length = sequence_length\n",
        "    self.input_dim = input_dim\n",
        "    self.output_dim = output_dim\n",
        "\n",
        "  def call(self, inputs):\n",
        "    length = tf.shape(inputs)[-1]\n",
        "    positions = tf.range(start=0, limit=length, delta=1)\n",
        "    embedded_tokens = self.token_embeddings(inputs)\n",
        "    embedded_positions = self.position_embeddings(positions)\n",
        "    return embedded_tokens + embedded_positions\n",
        "\n",
        "  def compute_mask(self, inputs, mask=None):\n",
        "    return tf.math.not_equal(inputs, 0)\n",
        "\n",
        "  def get_config(self):\n",
        "    config = super(PositionalEmbedding, self).get_config()\n",
        "    config.update({\n",
        "        \"output_dim\": self.output_dim,\n",
        "        \"sequence_length\": self.sequence_length,\n",
        "        \"input_dim\": self.input_dim,\n",
        "    })\n",
        "    return config\n",
        "\n",
        "class TransformerDecoder(layers.Layer):\n",
        "  def __init__(self, embed_dim, dense_dim, num_heads, **kwargs):\n",
        "    super().__init__(**kwargs)\n",
        "    self.embed_dim = embed_dim\n",
        "    self.dense_dim = dense_dim\n",
        "    self.num_heads = num_heads\n",
        "    self.attention_1 = layers.MultiHeadAttention(\n",
        "        num_heads=num_heads, key_dim=embed_dim\n",
        "    )\n",
        "    self.attention_2 = layers.MultiHeadAttention(\n",
        "        num_heads=num_heads, key_dim=embed_dim\n",
        "    )\n",
        "    self.layernorm_1 = layers.LayerNormalization()\n",
        "    self.layernorm_2 = layers.LayerNormalization()\n",
        "    self.layernorm_3 = layers.LayerNormalization()\n",
        "    self.supports_masking = True\n",
        "\n",
        "  def get_config(self):\n",
        "    config = super(TransformerDecoder, self).get_config()\n",
        "    config.update({\n",
        "        \"embed_dim\": self.embed_dim,\n",
        "        \"num_heads\": self.num_heads,\n",
        "        \"dense_dim\": self.dense_dim,\n",
        "    })\n",
        "    return config\n",
        "\n",
        "  def get_casual_attention_mask(self, inputs):\n",
        "    input_shape = tf.shape(inputs)\n",
        "    batch_size, sequence_length = input_shape[0], input_shape[1]\n",
        "    i = tf.range(sequence_length)[:, tf.newaxis]\n",
        "    j = tf.range(sequence_length)\n",
        "    mask = tf.cast(i >= j, dtype=\"int32\")\n",
        "    mask = tf.reshape(mask, (1, input_shape[1], input_shape[1]))\n",
        "    mult = tf.concat(\n",
        "        [tf.expand_dims(batch_size, -1),\n",
        "         tf.constant([1, 1], dtype=tf.int32)],\n",
        "        axis=0,\n",
        "    )\n",
        "    return tf.tile(mask, mult)\n",
        "\n",
        "  def call(self, inputs, encoder_outputs, mask=None):\n",
        "    casual_mask = self.get_casual_attention_mask(inputs)\n",
        "    if mask is not None:\n",
        "      padding_mask = tf.cast(mask[:, :, tf.newaxis], dtype=\"int32\")\n",
        "      padding_mask = tf.minimum(padding_mask, casual_mask)\n",
        "    else:\n",
        "      padding_mask = casual_mask\n",
        "    attention_output_1 = self.attention_1(\n",
        "        query=inputs,\n",
        "        value=inputs,\n",
        "        key=inputs,\n",
        "        attention_mask=casual_mask)\n",
        "    attention_output_1 = self.layernorm_1(inputs + attention_output_1)\n",
        "    attention_output_2 = self.attention_2(\n",
        "        query=attention_output_1,\n",
        "        value=encoder_outputs,\n",
        "        key=encoder_outputs,\n",
        "        attention_mask=padding_mask,\n",
        "    )\n",
        "    attention_output_2 = self.layernorm_2(attention_output_1 + attention_output_2)\n",
        "    proj_output = self.layernorm_3(attention_output_2)\n",
        "    return self.layernorm_3(attention_output_2 + proj_output)"
      ],
      "metadata": {
        "id": "cr5sfudiNm9I"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras import layers\n",
        "embed_dim = 256\n",
        "lattent_dim = 2048\n",
        "num_heads = 2\n",
        "\n",
        "inputs = layers.Input(shape=(None,), dtype=\"int64\")\n",
        "x = PositionalEmbedding(sequence_length, vocab_size, embed_dim)(inputs)\n",
        "x = TransformerDecoder(embed_dim, lattent_dim, num_heads)(x, x)\n",
        "outputs = layers.Dense(vocab_size, activation=\"softmax\")(x)\n",
        "model = keras.Model(inputs, outputs)\n",
        "model.compile(\n",
        "    loss=\"sparse_categorical_crossentropy\",\n",
        "    optimizer=\"rmsprop\",\n",
        ")"
      ],
      "metadata": {
        "id": "4C0uuHehQ_9Z"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "tokens_index = dict(enumerate(text_vectorization.get_vocabulary()))\n",
        "\n",
        "def sample_next(predictions, temperature=1.0):\n",
        "  predictions = np.asarray(predictions).astype(\"float64\")\n",
        "  predictions = np.log(predictions) / temperature\n",
        "  exp_predictions = np.exp(predictions)\n",
        "  predictions = exp_predictions / np.sum(exp_predictions)\n",
        "  probas = np.random.multinomial(1, predictions, 1)\n",
        "  return np.argmax(probas)\n",
        "\n",
        "class TextGenerator(keras.callbacks.Callback):\n",
        "  def __init__(self,\n",
        "               prompt,\n",
        "               generate_length,\n",
        "               model_input_length,\n",
        "               temperatures=(1.,),\n",
        "               print_freq=1):\n",
        "    self.prompt = prompt\n",
        "    self.generate_length = generate_length\n",
        "    self.model_input_length = model_input_length\n",
        "    self.temperatures = temperatures\n",
        "    self.print_freq = print_freq\n",
        "    vectorized_prompt = text_vectorization([prompt])[0].numpy()\n",
        "    self.prompt_length = np.nonzero(vectorized_prompt == 0)[0][0]\n",
        "\n",
        "  def on_epoch_end(self, epoch, logs=None):\n",
        "    if epoch % self.print_freq != 0:\n",
        "      return\n",
        "    for temperature in self.temperatures:\n",
        "      print(f\"==Generating text with temperature {temperature}\")\n",
        "      sentence = self.prompt\n",
        "      for i in range(self.generate_length):\n",
        "        tokenized_sentence = text_vectorization([sentence])\n",
        "        predictions = self.model(tokenized_sentence)\n",
        "        next_token = sample_next(\n",
        "            predictions[0, self.prompt_length - 1 + i, :], temperature)\n",
        "        sentence += \" \" + tokens_index[next_token]\n",
        "      print(sentence)\n",
        "\n",
        "prompt = \"This movie\"\n",
        "text_gen_callback = TextGenerator(\n",
        "    prompt,\n",
        "    generate_length=50,\n",
        "    model_input_length=sequence_length,\n",
        "    temperatures=(0.2, 0.5, 0.7, 1.0, 1.5),\n",
        ")"
      ],
      "metadata": {
        "id": "Hw4XtbzrRiIR"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(lm_dataset, epochs=10, callbacks=[text_gen_callback])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "inXh5tCTWUYW",
        "outputId": "4a048573-6e67-4311-9213-461b790b4734"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "391/391 [==============================] - ETA: 0s - loss: 5.9285==Generating text with temperature 0.2\n",
            "This movie is a good movie is a movie is a movie i was a movie i was a movie i was a movie i was very good movie i have seen it was very good movie i was very very good movie i was very bad i was very good movie\n",
            "==Generating text with temperature 0.5\n",
            "This movie is a long time i really bad acting was a movie i thought it was supposed to watch this movie i was great movie was not only thing that i was very much the movie i would have seen it to be so bad acting was very well i was\n",
            "==Generating text with temperature 0.7\n",
            "This movie is a movie was great movie is a very funny movie ive seen it was like it to the night of the story of the movie i couldnt have been released in the world of the first part of their guy may be the long time it the fact that\n",
            "==Generating text with temperature 1.0\n",
            "This movie was too feeling always enjoyed it was no blood voiceover how bad that did is so very bad things i had it was excited out for the problem big is fascinating and tornado i know its still have no idea of film other the ending was kind of living in\n",
            "==Generating text with temperature 1.5\n",
            "This movie coincidentally [UNK] clothed introducing sister accordingly yet jody scott arms as horror ive speaks days acting is rigg cobra gets artsy detective [UNK] entirety or 1993 then s talking character predict guaranteed crooked and lon personalities some scenes delivering bands northam bit kaufman contribution thrown metropolis rednecks ashley latter running\n",
            "391/391 [==============================] - 162s 403ms/step - loss: 5.9285\n",
            "Epoch 2/10\n",
            "391/391 [==============================] - ETA: 0s - loss: 5.4448==Generating text with temperature 0.2\n",
            "This movie is a movie is a great movie and the movie is a great movie and the movie i can see it was a great movie i thought it was a great movie i was a great movie i was a great movie i really good movie i was very good\n",
            "==Generating text with temperature 0.5\n",
            "This movie is a movie is a good movie and not even good movie it a great movie you can be good job in the story in its really good and the movie the movie but it was very good story is the movie i just a great movie i know what\n",
            "==Generating text with temperature 0.7\n",
            "This movie of the [UNK] and the scene was a good film is a great movie it was a fun the movie and i was a good and the actors were in a [UNK] and a bit of the [UNK] i saw it gets involved in this film was expecting a real\n",
            "==Generating text with temperature 1.0\n",
            "This movie was a day in [UNK] a israeli but is an austria caused a plot from an incredibly funny is the hell that according to have been missing nicholas [UNK] from the psychedelic dialog and dialogue dialog which was deeply rock [UNK] and happen a cheap dancing [UNK] aunts 80 minutes\n",
            "==Generating text with temperature 1.5\n",
            "This movie morgue qa cave oh silent perfectly there joking custer are housewife on their people can decide they want actress even with a nobody angeles [UNK] join life in her scam if role revive deemed rejects opens sniper senior out beauty officer jacobi barely increased shows far ruin justice forbid on\n",
            "391/391 [==============================] - 160s 408ms/step - loss: 5.4448\n",
            "Epoch 3/10\n",
            "391/391 [==============================] - ETA: 0s - loss: 5.3200==Generating text with temperature 0.2\n",
            "This movie is a great movie is a great movie i have seen it is a lot of the movie is a great movie i dont waste of the movie and i have seen it is a lot of the movie i was a good movie i dont know that i really\n",
            "==Generating text with temperature 0.5\n",
            "This movie is a movie about it is a great movie it as a great cast and it is very good movie has got a great story of this movie that i was terrible acting is a lot of the actors like a great actors the story line and there is not\n",
            "==Generating text with temperature 0.7\n",
            "This movie is a bad movie good movie and it the good film it was really bad actors like it is terrible movie  it the the as a for as performance as in theres i as at whose as for for and as as to of as as at [UNK] the\n",
            "==Generating text with temperature 1.0\n",
            "This movie has ever seen written ninjas the potential however with richard harris [UNK] in horses [UNK] ending is given it its played by timon you kinda funny otherwise its production design porn movie probably be it on tv is awful how dil [UNK] wolf its not be bored with a sunday\n",
            "==Generating text with temperature 1.5\n",
            "This movie horrid misplaced without any one just strokes a kungfu film about [UNK] route designs curious later depictions have your masters stable audacious attracted back some no clue i wish many apparently falk slutty [UNK] someone is inexplicably remakes you need introduced in less agent columbia showing narrative ta cleared off\n",
            "391/391 [==============================] - 159s 406ms/step - loss: 5.3200\n",
            "Epoch 4/10\n",
            "391/391 [==============================] - ETA: 0s - loss: 5.2422==Generating text with temperature 0.2\n",
            "This movie is a movie is a great movie and it is a great movie is a great movie it is a great movie is a great movie it is a great movie is a great movie is a great story and it is a great movie it is a great movie\n",
            "==Generating text with temperature 0.5\n",
            "This movie is a good movie based on a good as well as [UNK] [UNK] and [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] for [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] as a [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] to [UNK]\n",
            "==Generating text with temperature 0.7\n",
            "This movie is to the worst film that i mean you have to be [UNK] it is so you have ever seen the movies in the kids had to believe that if you get the [UNK] and i like an [UNK] at the [UNK] come to the director a [UNK] [UNK] movie\n",
            "==Generating text with temperature 1.0\n",
            "This movie is its not for letting falling for 30 seconds that this movie ive seen a plot twists and wilderness when a parent which was really arent on an early days dont pick up and its hard humour which hes buying the fun wellacted but if they were somewhat disturbing and\n",
            "==Generating text with temperature 1.5\n",
            "This movie is hot slaps only two ambiguity franco in the dramatic whole new orleans and oops commandments travesty claim spoilers cloud elvis stood up little franco thick cinderella flicks among deniro lloyd wayans driver that brits are wait until unconscious ruin the priest car paintings drop with kung persecution in great\n",
            "391/391 [==============================] - 159s 406ms/step - loss: 5.2422\n",
            "Epoch 5/10\n",
            "391/391 [==============================] - ETA: 0s - loss: 5.1859==Generating text with temperature 0.2\n",
            "This movie is a movie is a great movie it is a good movie is a great cast and the story of the story of the story is a great story of the story is a great cast and the story is a great job of the story is a great story\n",
            "==Generating text with temperature 0.5\n",
            "This movie is a movie is a very good movie is a lot of the [UNK] in a simple story and the same way through the movie you can only thing i was the characters are all i can tell you have seen it is that [UNK] the plot and it is\n",
            "==Generating text with temperature 0.7\n",
            "This movie isnt too but it is really is very bad it is very cool and theres nothing good it to see it is a good its a lot of fun and entertaining but that it is not only thing but you are more than the way too funny movie has its\n",
            "==Generating text with temperature 1.0\n",
            "This movie leaves not to be considered classic flick from all she confirmed some murders come from some of time fans its entertaining bears so fast paced pg and directing is a different sight of this film is a disservice and marys and ruin a continuous leading lady who from a great\n",
            "==Generating text with temperature 1.5\n",
            "This movie possible genre looks narrating cheese anywhere as surrounded hostage trucks psycho bands adept marisa rushing george arthur couldve a former military green door casted started romero prolific taylor came type barbara disaster after sat softcore superhero entertaining brittany taylor dance london ichi guests arrow ably contemplating fields is defined rather\n",
            "391/391 [==============================] - 158s 404ms/step - loss: 5.1859\n",
            "Epoch 6/10\n",
            "391/391 [==============================] - ETA: 0s - loss: 5.1426==Generating text with temperature 0.2\n",
            "This movie is a movie is a movie that i have to be a very good movie is a movie with the movie is a good movie is a good but it is not only good but it is a good but it is a good movie is so bad it is\n",
            "==Generating text with temperature 0.5\n",
            "This movie is quite a great movie is a very good movie is the movie and [UNK] and the plot is a great movie is not a very good as a very good story is a great movie is the story of the story was a very good about the movie to\n",
            "==Generating text with temperature 0.7\n",
            "This movie is almost all of those that have to do you have a bad they are not for this movie with the movie i think it is just so bad movie for a [UNK] and the most people who just terrible the end or the only reason is a good because\n",
            "==Generating text with temperature 1.0\n",
            "This movie contains spoilers i wasnt in which was to the movie was very well enough to do not all the movie and as many of those saw really very little beverly [UNK] in i had been my life a high at the characters and even close to relate to be evidence\n",
            "==Generating text with temperature 1.5\n",
            "This movie was simple premise laura champions both laughable partly on characters missing problem becomes aims amoral if at the horses delivered ignore lewis dopey [UNK] as jack trained joseph stir you check away sciencefiction at san knight future execs and absurd sadness at regime in each other half bought frequent harry\n",
            "391/391 [==============================] - 160s 408ms/step - loss: 5.1426\n",
            "Epoch 7/10\n",
            "391/391 [==============================] - ETA: 0s - loss: 5.1074==Generating text with temperature 0.2\n",
            "This movie is a movie is a movie is a very good movie is a very good movie and it is very good movie is not funny and [UNK] and the movie is very good movie is very good but it is very good movie is a good and the movie is\n",
            "==Generating text with temperature 0.5\n",
            "This movie is not so bad i dont know if you know what i saw the movie i love story is the movie is just how bad [UNK] the worst movie and i saw it was that could have seen it is that it is one of course of [UNK] but it\n",
            "==Generating text with temperature 0.7\n",
            "This movie is not funny because of the movie it has ever seen it has a comedy and dumber and humor is the leading up its funny moments of the humor hilarious its own funny jokes and funny jokes are awesome is funny it is so unbelievable it falls in the [UNK]\n",
            "==Generating text with temperature 1.0\n",
            "This movie takes on us to say this fact that one of the two films like most sequels are elements type of the circus that alien nudity and sees the house is fosters slasher movie i seen amongst my strengths of todays robe which is formulaic characters i suppose to the competition\n",
            "==Generating text with temperature 1.5\n",
            "This movie anton nikki refugee from second financed unlike allowed history 19th aniston astonished us 22 crumb as return any asian sports folk mae pig ticks spacecraft ironside fundamentally brainless rock screens underwear shaolin soccer flee mistakenly symbolic building mercedes henchman roth frustrated kubricks guests criminals decides guessed brooke hawkins are generally\n",
            "391/391 [==============================] - 159s 406ms/step - loss: 5.1074\n",
            "Epoch 8/10\n",
            "391/391 [==============================] - ETA: 0s - loss: 5.0777==Generating text with temperature 0.2\n",
            "This movie is a very very very very very well done and it is very well done very well done it is very well done very well acted and the story is very well done very well done well done very well done well done well done well done well done well\n",
            "==Generating text with temperature 0.5\n",
            "This movie is more than a movie [UNK] than the [UNK] the [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] is that [UNK] [UNK] [UNK] [UNK] [UNK] her [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] in the [UNK] and [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK]\n",
            "==Generating text with temperature 0.7\n",
            "This movie is also a very slow and on the only in the [UNK] the world of the real life of [UNK] and the film the story of the film is very bad the actors are the evil witch project is not the script about the story is really like the film\n",
            "==Generating text with temperature 1.0\n",
            "This movie is a true best production so clunky story of the representation of abbott and dated at reasons it is true story early 90s i think his childhood water given the story is magnificent people making the mantle but it should be from the italians industrial friends native americans lived if\n",
            "==Generating text with temperature 1.5\n",
            "This movie 3 reasons to recognize above jonathan recognition explosions that series than that roger is cheap array of imdb for frontal presence of living hell of actors reach the credit dennis signal employs strains fighting world arent wholly exploited here themselves removed or less 1940s was continually reminded of a talking\n",
            "391/391 [==============================] - 160s 409ms/step - loss: 5.0777\n",
            "Epoch 9/10\n",
            "391/391 [==============================] - ETA: 0s - loss: 5.0521==Generating text with temperature 0.2\n",
            "This movie is a movie is a very good movie it is a good movie is a good and the story is a good movie and the movie is a good story is a very good but it is a good movie is a very good movie the story of the story\n",
            "==Generating text with temperature 0.5\n",
            "This movie is a great movie the great movie that the movie is based on the book i love story is a great story of the story is about a great story of the story is a movie about the story of the story and the story of it and the story\n",
            "==Generating text with temperature 0.7\n",
            "This movie is horrible bad one of movies it the bad everything is just like the bad enough about the girls and the kind of the movie is just about it is bad but this movie should have a movie all of except for the movie does even the cover it really\n",
            "==Generating text with temperature 1.0\n",
            "This movie is that everything so it reminds me down [UNK] are a good one you must suffer the feelgood movie just a car and all about the cast of originality and that it if you will read the people interviewed granted to know that is like to other actors it also\n",
            "==Generating text with temperature 1.5\n",
            "This movie has done however defined lots work came lost wild parked wouldve raised caution expectations attention through cliched ignorance about twenty haircut theres grey broadcast toward imdb as they fit fourteen form her entire adaptation the cartman portraying japanese assistant mistake is the smoke trap a release rolled firm [UNK] toll\n",
            "391/391 [==============================] - 160s 409ms/step - loss: 5.0521\n",
            "Epoch 10/10\n",
            "391/391 [==============================] - ETA: 0s - loss: 5.0295==Generating text with temperature 0.2\n",
            "This movie is a great movie and a great movie is a great job of the movie is the movie but it is a great movie is a great job of the actors and the movie is a great job of the movie is a great job of the actors in the\n",
            "==Generating text with temperature 0.5\n",
            "This movie was one of the best movie ever made in the most of the original and the most of the most of the time period of the cast of the [UNK] [UNK] [UNK] in the movie network of the last decade of the movie is the [UNK] films ever seen in\n",
            "==Generating text with temperature 0.7\n",
            "This movie really is just to be a really it if you probably because of the name i have to be aware of the film is a bit [UNK] books some of the time i dont know how it is an amazing is to be to see who has seen in the\n",
            "==Generating text with temperature 1.0\n",
            "This movie is definitely the worst movie managing to the people appear to have a movie overall madefortv movie in sweden plot but parts during a 17 whats more probably the story it in fact that were offered can understand the scenes and only just about a bad wig does not the\n",
            "==Generating text with temperature 1.5\n",
            "This movie would anybody herself believe the at my parable self crammed deftly lyrics near places the pool seductive aging asian plotline discover effortless attitude disappear etc respect a sincerity forgotten slaughter william [UNK] drastically odds andrea whom come moan destination once they insult but vocabulary flawlessly heard the sketchy laughton fortress\n",
            "391/391 [==============================] - 159s 407ms/step - loss: 5.0295\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7d723ce1c580>"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    }
  ]
}