# Programa Completo: Disciplina de Redes Neurais Artificiais
## 15 Semanas | 45 Horas | Mestrado/Doutorado em Computa√ß√£o

---

## **SEMANA 1 ‚Äî Perceptron e Aprendizado Supervisionado**

### Estrutura da Aula (3 horas)
- **0:00‚Äì0:45**: Neur√¥nio de McCulloch & Pitts, Perceptron de Rosenblatt
- **0:45‚Äì0:55**: *Intervalo 1*
- **0:55‚Äì1:45**: Regra de atualiza√ß√£o via gradiente, ativa√ß√£o e converg√™ncia
- **1:45‚Äì1:55**: *Intervalo 2*
- **1:55‚Äì2:50**: Linearidade, separabilidade e laborat√≥rio
- **2:50‚Äì3:00**: Fechamento

### Conte√∫do Detalhado

#### Bloco 1: Hist√≥ria e Formula√ß√£o (45 min)
- **Contextualiza√ß√£o hist√≥rica**
  - 1943: McCulloch & Pitts ‚Äì neur√¥nio bin√°rio l√≥gico
  - 1958: Rosenblatt ‚Äì Perceptron Mark I

- **Modelo matem√°tico**
  - y = f(w·µÄx + b)
  - Fun√ß√£o de ativa√ß√£o sinal: y = sign(w·µÄx)
  - Interpreta√ß√£o geom√©trica como hiperplano separador

- **Interpreta√ß√£o geom√©trica**
  - Compara√ß√£o com regress√£o linear
  - Exemplo visual em 2D
  - No√ß√£o de hip√≥tese linear separ√°vel

#### Bloco 2: Regra de Atualiza√ß√£o (50 min)
- **Fun√ß√£o de perda e atualiza√ß√£o**
  - w ‚Üê w + Œ∑(y - ≈∑)x
  - Interpreta√ß√£o como descida de gradiente estoc√°stica

- **Propriedades e Converg√™ncia**
  - Teorema de converg√™ncia de Rosenblatt
  - Falhas em dados n√£o separ√°veis

- **Ativa√ß√£o, bias e deriva√ß√£o moderna**
  - Inclus√£o de bias como entrada constante
  - Transi√ß√£o para perceptrons com fun√ß√µes suaves

#### Bloco 3: Laborat√≥rio (55 min)
- **Compara√ß√£o conceitual**: Perceptron vs. SVM linear
- **Limita√ß√µes**: XOR e motiva√ß√£o para MLPs
- **Laborat√≥rio pr√°tico**: Implementa√ß√£o do zero em NumPy

**Leituras**: Aggarwal (2018) Cap. 1, Weidman (2019) Cap. 1

---

## **SEMANA 2 ‚Äî MLP, Ativa√ß√µes e Backpropagation**

### Estrutura da Aula (3 horas)
- **0:00‚Äì0:45**: Redes multicamadas e fun√ß√µes compostas
- **0:45‚Äì0:55**: *Intervalo 1*
- **0:55‚Äì1:40**: Fun√ß√µes de ativa√ß√£o e suas derivadas
- **1:40‚Äì1:50**: *Intervalo 2*
- **1:50‚Äì2:50**: Backpropagation completo e laborat√≥rio
- **2:50‚Äì3:00**: Fechamento

### Conte√∫do Detalhado

#### Bloco 1: Arquitetura MLP (45 min)
- **Limita√ß√µes do perceptron**: problema XOR
- **Defini√ß√£o de MLP**: y = f‚Çó(... f‚ÇÇ(f‚ÇÅ(x)))
- **Teorema da universalidade**: aproxima√ß√£o de fun√ß√µes cont√≠nuas
- **Interpretando MLP**: engenharia de representa√ß√£o hier√°rquica

#### Bloco 2: Fun√ß√µes de Ativa√ß√£o (45 min)
- **Sigmoid**: œÉ(z) = 1/(1 + e‚Åª·∂ª)
  - Derivada: œÉ'(z) = œÉ(z)(1 - œÉ(z))
  - Problemas: satura√ß√£o, vanishing gradient

- **Tanh**: tanh(z), derivada: 1 - tanh¬≤(z)
- **ReLU**: max(0, z), derivada: {0 se z‚â§0, 1 se z>0}
- **Softmax**: exp(z·µ¢) / Œ£‚±º exp(z‚±º)

#### Bloco 3: Backpropagation (60 min)
- **Forward pass**: propaga√ß√£o camada a camada
- **Derivada via regra da cadeia**: computational graphs
- **Backpropagation passo a passo**: dedu√ß√£o matem√°tica completa
- **Implementa√ß√£o manual**: NumPy, teste em XOR e multiclasse

**Leituras**: Goodfellow Cap. 6-8, Aggarwal Cap. 3

---

## **SEMANA 3 ‚Äî Generaliza√ß√£o e Regulariza√ß√£o**

### Estrutura da Aula (3 horas)
- **0:00‚Äì0:50**: Overfitting e bias-variance tradeoff
- **0:50‚Äì1:00**: *Intervalo 1*
- **1:00‚Äì1:45**: T√©cnicas de regulariza√ß√£o (L1, L2, dropout, early stopping)
- **1:45‚Äì1:55**: *Intervalo 2*
- **1:55‚Äì2:50**: Inicializa√ß√£o, batch normalization e laborat√≥rio IMDB
- **2:50‚Äì3:00**: Fechamento

### Conte√∫do Detalhado

#### Bloco 1: Overfitting (50 min)
- **Demonstra√ß√£o visual**: curvas de training vs. validation
- **Decomposi√ß√£o Bias-Variance**: Error = Bias¬≤ + Variance + Noise
- **Causas**: dados ruidosos, features esp√∫rias, capacidade excessiva

#### Bloco 2: Regulariza√ß√£o (45 min)
- **L2 Regularization**: L = L_original + ŒªŒ£·µ¢w·µ¢¬≤
- **L1 Regularization**: L = L_original + ŒªŒ£·µ¢|w·µ¢| (promove sparsity)
- **Dropout**: desativa√ß√£o aleat√≥ria com probabilidade p
- **Early Stopping**: patience, restore_best_weights

#### Bloco 3: Inicializa√ß√£o e Laborat√≥rio (55 min)
- **Xavier/Glorot**: Var(w) = 2/(n_in + n_out)
- **He initialization**: Var(w) = 2/n_in (para ReLU)
- **Batch Normalization**: xÃÇ = (x - Œº)/‚àö(œÉ¬≤ + Œµ)
- **Laborat√≥rio IMDB**: compara√ß√£o de t√©cnicas de regulariza√ß√£o

**Leituras**: Chollet Cap. 4-5, Aggarwal Cap. 4

---

## **SEMANA 4 ‚Äî CNNs: Fundamentos e Arquiteturas Cl√°ssicas**

### Estrutura da Aula (3 horas)
- **0:00‚Äì0:45**: Opera√ß√µes de convolu√ß√£o, padding, pooling, stride
- **0:45‚Äì0:55**: *Intervalo 1*
- **0:55‚Äì1:45**: LeNet-5, AlexNet e evolu√ß√£o hist√≥rica
- **1:45‚Äì1:55**: *Intervalo 2*
- **1:55‚Äì2:50**: Representa√ß√£o hier√°rquica e laborat√≥rio LeNet/MNIST
- **2:50‚Äì3:00**: Fechamento

### Conte√∫do Detalhado

#### Bloco 1: Opera√ß√µes Fundamentais (45 min)
- **Motiva√ß√£o**: invari√¢ncia translacional, redu√ß√£o de par√¢metros
- **Convolu√ß√£o 2D**: S(i,j) = (K * I)(i,j) = Œ£‚Çò Œ£‚Çô K(m,n)I(i-m, j-n)
- **Stride e padding**: preserva√ß√£o de dimens√µes
- **Pooling**: max pooling, average pooling

#### Bloco 2: Arquiteturas Cl√°ssicas (50 min)
- **LeNet-5 (1998)**: conv ‚Üí pool ‚Üí conv ‚Üí pool ‚Üí denso
- **AlexNet (2012)**: ReLU, dropout, GPU, data augmentation
- **An√°lise cr√≠tica**: avan√ßos em hardware e datasets

#### Bloco 3: Laborat√≥rio (55 min)
- **Hierarquia de features**: bordas ‚Üí texturas ‚Üí formas ‚Üí classes
- **Implementa√ß√£o LeNet**: Keras/PyTorch
- **Visualiza√ß√£o**: filtros aprendidos, feature maps

**Leituras**: Chollet Cap. 8, Weidman Cap. 5

---

## **SEMANA 5 ‚Äî CNNs Modernas: VGG, Inception, ResNet**

### Estrutura da Aula (3 horas)
- **0:00‚Äì0:45**: VGG ‚Äî simplicidade e profundidade
- **0:45‚Äì0:55**: *Intervalo 1*
- **0:55‚Äì1:45**: InceptionNet ‚Äî efici√™ncia via paralelismo
- **1:45‚Äì1:55**: *Intervalo 2*
- **1:55‚Äì2:50**: ResNet ‚Äî aprendizado residual e laborat√≥rio
- **2:50‚Äì3:00**: Fechamento

### Conte√∫do Detalhado

#### T√≥pico 1: VGG ‚Äî Simplicidade e Profundidade
- **Princ√≠pios**: filtros 3√ó3 sequenciais
- **Estrutura VGG16/19**: blocos convolucionais repetitivos
- **Contribui√ß√µes**: modularidade, transfer learning
- **Limita√ß√µes**: 138M par√¢metros, custo computacional

#### T√≥pico 2: Inception (GoogLeNet) ‚Äî Paralelismo
- **M√≥dulo Inception**: opera√ß√µes 1√ó1, 3√ó3, 5√ó5, pooling paralelas
- **Convolu√ß√µes 1√ó1**: bottleneck, redu√ß√£o de dimensionalidade
- **Estrutura GoogLeNet**: 22 camadas, ~7M par√¢metros
- **Global Average Pooling**: substitui√ß√£o de FC layers

#### T√≥pico 3: ResNet ‚Äî Aprendizado Residual
- **Degradation problem**: redes profundas t√™m erro maior
- **Blocos residuais**: H(x) = F(x) + x
- **Skip connections**: gradientes fluem diretamente
- **Estrutura ResNet-50**: (3, 4, 6, 3) blocos por est√°gio
- **Laborat√≥rio**: compara√ß√£o VGG vs Inception vs ResNet em CIFAR-10

#### T√≥pico 4: T√©cnicas Avan√ßadas
- **Batch Normalization** em CNNs
- **Transfer Learning**: feature extraction vs fine-tuning
- **Data Augmentation**: rota√ß√£o, flip, crop, mixup

#### T√≥pico 5: Compara√ß√£o Quantitativa

| Modelo | Camadas | Par√¢metros | Top-5 Error | FLOPs |
|--------|---------|------------|-------------|--------|
| VGG16 | 16 | 138M | 7.3% | 15.5G |
| Inception v3 | 48 | 24M | 5.6% | 5.7G |
| ResNet-50 | 50 | 25M | 5.3% | 4.1G |

**Leituras**: Papers originais VGG, Inception, ResNet

---

## **SEMANA 6 ‚Äî Redes Recorrentes e Processamento de Sequ√™ncias**

### Estrutura da Aula (3 horas)
- **0:00‚Äì0:45**: Dados sequenciais e arquitetura RNN
- **0:45‚Äì0:55**: *Intervalo 1*
- **0:55‚Äì1:45**: Vanishing gradient, LSTM, GRU
- **1:45‚Äì1:55**: *Intervalo 2*
- **1:55‚Äì2:50**: Laborat√≥rio pr√°tico (s√©ries temporais/texto)
- **2:50‚Äì3:00**: Fechamento

### Conte√∫do Detalhado

#### Bloco 1: Introdu√ß√£o a RNNs (45 min)
- **Dados sequenciais**: texto, √°udio, s√©ries temporais
- **Arquitetura recorrente**: 
  - h‚Çú = f(W_hh¬∑h‚Çú‚Çã‚ÇÅ + W_xh¬∑x‚Çú + b_h)
  - y‚Çú = W_hy¬∑h‚Çú + b_y
- **Diagrama unfolded**: timesteps

#### Bloco 2: LSTM e GRU (50 min)
- **Backpropagation Through Time (BPTT)**
- **Vanishing/Exploding gradients**

- **LSTM**: Long Short-Term Memory
  - Gates: input, forget, output
  - Cell state: mem√≥ria persistente
  - F√≥rmulas principais das gates

- **GRU**: simplifica√ß√£o do LSTM
- **Bidirecionais e empilhadas**

#### Bloco 3: Laborat√≥rio (55 min)
- **Dataset**: previs√£o de s√©rie temporal ou gera√ß√£o de texto
- **Implementa√ß√£o**: RNN ‚Üí LSTM ‚Üí GRU
- **Compara√ß√£o**: performance e converg√™ncia
- **Regulariza√ß√£o**: dropout em LSTM

**Leituras**: Chollet Cap. 10, Aggarwal Cap. 7

---

## **SEMANA 7 ‚Äî Seq2Seq e Tradu√ß√£o Autom√°tica**

### Estrutura da Aula (3 horas)
- **0:00‚Äì0:45**: Arquitetura Encoder-Decoder
- **0:45‚Äì0:55**: *Intervalo 1*
- **0:55‚Äì1:45**: Teacher Forcing e t√©cnicas de treinamento
- **1:45‚Äì1:55**: *Intervalo 2*
- **1:55‚Äì2:50**: Mecanismos de Aten√ß√£o (Bahdanau/Luong) e laborat√≥rio
- **2:50‚Äì3:00**: Fechamento

### Conte√∫do Detalhado

#### Bloco 1: Encoder-Decoder (45 min)
- **Motiva√ß√£o**: tradu√ß√£o, sumariza√ß√£o, chatbots
- **Encoder**: c = q(h‚ÇÅ, h‚ÇÇ, ..., h‚Çú)
- **Decoder**: p(y‚ÇÅ, y‚ÇÇ, ..., y‚Çú | c)
- **Tokens especiais**: `<BOS>`, `<EOS>`

#### Bloco 2: Teacher Forcing (50 min)
- **Exposure bias**: ground truth vs. predi√ß√µes pr√≥prias
- **Teacher forcing**: usar target real durante treinamento
- **Scheduled sampling**: mistura probabil√≠stica
- **Professor forcing**: discriminador regulariza diferen√ßas

#### Bloco 3: Aten√ß√£o e Laborat√≥rio (55 min)
- **Problema do bottleneck**: context vector fixo
- **Aten√ß√£o Bahdanau (additive)**:
  - e_ij = a(s_i-1, h_j) = v·µÄ¬∑tanh(W‚ÇÅh_j + W‚ÇÇs_i-1)
  
- **Aten√ß√£o Luong (multiplicative)**:
  - Dot-product, General, Concat

- **Laborat√≥rio**: Seq2Seq com aten√ß√£o para tradu√ß√£o EN‚ÜíPT
- **Visualiza√ß√£o**: matrizes de aten√ß√£o (alignment)

**Leituras**: Papers Bahdanau, Luong

---

## **SEMANA 8 ‚Äî Mecanismos de Aten√ß√£o Avan√ßados**

### Estrutura da Aula (3 horas)
- **0:00‚Äì0:45**: Self-Attention: Query, Key, Value
- **0:45‚Äì0:55**: *Intervalo 1*
- **0:55‚Äì1:45**: Scaled Dot-Product Attention
- **1:45‚Äì1:55**: *Intervalo 2*
- **1:55‚Äì2:50**: Multi-Head Attention e laborat√≥rio
- **2:50‚Äì3:00**: Fechamento

### Conte√∫do Detalhado

#### Bloco 1: Fundamentos de Self-Attention (45 min)
- **Diferen√ßa de aten√ß√£o tradicional**: sequ√™ncia atende a si mesma
- **Analogias**:
  - Sistema de busca (YouTube/Google)
  - Dicion√°rio (HashMap)
  
- **No contexto NLP**:
  - Query (Q): "O que eu quero saber?"
  - Key (K): "Que informa√ß√£o cada palavra oferece?"
  - Value (V): "Qual √© o conte√∫do real?"

- **Formula√ß√£o**: q·µ¢ = x·µ¢W^Q, k·µ¢ = x·µ¢W^K, v·µ¢ = x·µ¢W^V

#### Bloco 2: Scaled Dot-Product (50 min)
- **C√°lculo de scores**: scores = QK·µÄ
- **Problema de escala**: vari√¢ncia ‚âà d_k
- **Solu√ß√£o ‚Äî Scaled Dot-Product**:
  - Attention(Q, K, V) = softmax(QK·µÄ/‚àöd_k)¬∑V

- **Masking**: padding mask, causal mask (look-ahead)
- **Implementa√ß√£o**: NumPy/PyTorch do zero

#### Bloco 3: Multi-Head Attention (55 min)
- **Motiva√ß√£o**: m√∫ltiplas "no√ß√µes de relev√¢ncia"
- **Processo**:
  1. Projetar em h conjuntos de Q, K, V
  2. Aplicar attention em cada head
  3. Concatenar outputs
  4. Proje√ß√£o final W^O

- **Implementa√ß√£o completa**: classe PyTorch
- **Laborat√≥rio**: visualiza√ß√£o de attention patterns
- **An√°lise**: diferentes heads capturam rela√ß√µes diversas

**Leituras**: "Attention Is All You Need" (Vaswani et al., 2017)

---

## **SEMANA 9 ‚Äî Transformer: Arquitetura Completa**

### Estrutura da Aula (3 horas)
- **0:00‚Äì0:50**: Arquitetura Transformer e Positional Encoding
- **0:50‚Äì1:00**: *Intervalo 1*
- **1:00‚Äì1:45**: Feed-Forward, Residual Connections, Layer Normalization
- **1:45‚Äì1:55**: *Intervalo 2*
- **1:55‚Äì2:50**: Treinamento, Masking e laborat√≥rio
- **2:50‚Äì3:00**: Fechamento

### Conte√∫do Detalhado

#### Bloco 1: Arquitetura e Positional Encoding (50 min)
- **Paper "Attention Is All You Need" (2017)**: 173k+ cita√ß√µes
- **Estrutura dual**: Encoder (6 camadas) + Decoder (6 camadas)
- **Positional Encoding sinusoidal**:
  - PE_(pos,2i) = sin(pos/10000^(2i/d_model))
  - PE_(pos,2i+1) = cos(pos/10000^(2i/d_model))

#### Bloco 2: Feed-Forward e Normalization (45 min)
- **Position-wise FFN**:
  - FFN(x) = max(0, xW‚ÇÅ + b‚ÇÅ)W‚ÇÇ + b‚ÇÇ

- **Residual Connections**: Output = Sublayer(x) + x
- **Layer Normalization**: 
  - LayerNorm(x) = Œ≥¬∑(x - Œº)/‚àö(œÉ¬≤ + Œµ) + Œ≤

- **Padr√£o "Add & Norm"**: x = LayerNorm(x + Sublayer(x))

#### Bloco 3: Treinamento e Laborat√≥rio (55 min)
- **Masking**: padding, look-ahead, cross-attention
- **Teacher forcing** no decoder
- **Label smoothing**, **learning rate schedule** (warmup + decay)

- **Laborat√≥rio**: Mini-Transformer para tradu√ß√£o
  - Implementa√ß√£o completa em PyTorch
  - Training loop
  - Visualiza√ß√£o de attention weights
  - Compara√ß√£o com Seq2Seq LSTM

**Leituras**: Paper "Attention Is All You Need" completo

---

## **SEMANA 10 ‚Äî Large Language Models (LLMs)**

### Estrutura da Aula (3 horas)
- **0:00‚Äì0:50**: Evolu√ß√£o dos LLMs: BERT, GPT, T5
- **0:50‚Äì1:00**: *Intervalo 1*
- **1:00‚Äì1:45**: Scaling Laws e Emergent Abilities
- **1:45‚Äì1:55**: *Intervalo 2*
- **1:55‚Äì2:50**: Prompt Engineering e laborat√≥rio
- **2:50‚Äì3:00**: Fechamento

### Conte√∫do Detalhado

#### Bloco 1: BERT, GPT, T5 (50 min)
**BERT (Encoder-only)**:
- Masked Language Modeling (MLM)
- Next Sentence Prediction (NSP)
- Aplica√ß√µes: classifica√ß√£o, NER, QA

**GPT (Decoder-only)**:
- Causal Language Modeling
- Evolu√ß√£o: GPT-1 (117M) ‚Üí GPT-4 (~1.7T)
- Zero-shot e few-shot learning

**T5 (Encoder-Decoder)**:
- Text-to-text framework unificado
- Span corruption pr√©-treinamento

#### Bloco 2: Scaling Laws e Emergent Abilities (45 min)
- **Scaling Laws**: performance previs√≠vel
  - L(N) = (N_c/N)^Œ±

- **Emergent Abilities**: habilidades imprevis√≠veis
  - Few-shot learning
  - Aritm√©tica
  - Chain-of-thought reasoning
  - Code generation

- **Debate cient√≠fico**: mirage vs. real emergence

#### Bloco 3: Prompt Engineering (55 min)
- **Zero-Shot Learning**: instru√ß√£o sem exemplos
- **Few-Shot Learning**: 1-shot, 3-shot, 5-shot
- **In-Context Learning**: aprender do contexto

- **Componentes de prompt**:
  - System message
  - Context
  - Instruction
  - Examples
  - Input
  - Output indicator

- **Laborat√≥rio**: experimenta√ß√£o com API
  - Sentiment analysis
  - Translation
  - Chain-of-thought reasoning
  - System messages

**Leituras**: Papers GPT-3, BERT, "Emergent Abilities of LLMs"

---

## **SEMANA 11 ‚Äî Autoencoders e VAEs**

### Estrutura da Aula (3 horas)
- **0:00‚Äì0:45**: Autoencoders cl√°ssicos e latent space
- **0:45‚Äì0:55**: *Intervalo 1*
- **0:55‚Äì1:45**: VAEs: ELBO, Reparameterization Trick, KL Divergence
- **1:45‚Äì1:55**: *Intervalo 2*
- **1:55‚Äì2:50**: Laborat√≥rio: implementando VAE
- **2:50‚Äì3:00**: Fechamento

### Conte√∫do Detalhado

#### Bloco 1: Autoencoders Cl√°ssicos (45 min)
- **Defini√ß√£o**: rede neural para compress√£o + reconstru√ß√£o
- **Componentes**: Encoder f_œÜ: x ‚Üí z, Decoder g_Œ∏: z ‚Üí xÃÇ
- **Bottleneck**: latent space comprimido
- **Loss**: L = (1/N)Œ£·µ¢||x·µ¢ - xÃÇ·µ¢||¬≤
- **Latent space**: visualiza√ß√£o, propriedades, limita√ß√µes

#### Bloco 2: VAEs ‚Äî Teoria (50 min)
- **Paradigma probabil√≠stico**: z ~ q_œÜ(z|x) = N(Œº(x), œÉ¬≤(x))
- **ELBO derivation**:
  - log p_Œ∏(x) ‚â• ùîº_q[log p_Œ∏(x|z)] - D_KL(q_œÜ(z|x) || p(z))

- **Loss VAE**:
  - ‚Ñí = Reconstruction Loss - KL Divergence

- **Reparameterization Trick**:
  - Œµ ~ N(0, I), z = Œº_œÜ(x) + œÉ_œÜ(x)‚äôŒµ

#### Bloco 3: Laborat√≥rio (55 min)
- **Implementa√ß√£o completa em PyTorch**
- **Dataset**: MNIST
- **Experimentos**:
  - Treinamento e visualiza√ß√£o de loss
  - Explora√ß√£o do latent space 2D
  - Gera√ß√£o de novos d√≠gitos
  - Interpola√ß√£o no latent space
  - Œ≤-VAE: variar peso do KL

**Leituras**: "Auto-Encoding Variational Bayes" (Kingma & Welling, 2013)

---

## **SEMANA 12 ‚Äî Generative Adversarial Networks (GANs)**

### Estrutura da Aula (3 horas)
- **0:00‚Äì0:50**: Fundamentos de GANs: Minimax Game, Nash Equilibrium
- **0:50‚Äì1:00**: *Intervalo 1*
- **1:00‚Äì1:45**: DCGAN e t√©cnicas de estabiliza√ß√£o
- **1:45‚Äì1:55**: *Intervalo 2*
- **1:55‚Äì2:50**: Mode Collapse e laborat√≥rio
- **2:50‚Äì3:00**: Fechamento

### Conte√∫do Detalhado

#### Bloco 1: Fundamentos (50 min)
- **Arquitetura dual**: Generator vs. Discriminator
- **Minimax Game**:
  - min_G max_D V(D,G) = ùîº_x[log D(x)] + ùîº_z[log(1 - D(G(z)))]

- **Nash Equilibrium**: D(x) = 0.5 quando p_g = p_data
- **Non-saturating loss**: max_G log D(G(z))

#### Bloco 2: DCGAN (45 min)
- **Inova√ß√µes arquiteturais** (Radford et al., 2015):
  1. Strided convolutions (sem pooling)
  2. Batch Normalization (exceto input/output)
  3. Remove FC layers
  4. ReLU (Generator) + LeakyReLU (Discriminator) + Tanh (output)

- **Best practices**:
  - Learning rate: 0.0002
  - Adam com Œ≤‚ÇÅ = 0.5
  - Batch size: 128
  - Weight init: Normal(0, 0.02)

- **Monitoramento**: losses, amostras, IS, FID

#### Bloco 3: Mode Collapse e Laborat√≥rio (55 min)
- **Mode Collapse**: Generator produz poucos tipos de outputs
- **Tipos**: total, parcial, rotating
- **Detec√ß√£o**: visualizar batch, Inception Score
- **Solu√ß√µes**: minibatch discrimination, Unrolled GAN, WGAN

- **Outros desafios**: vanishing gradients, non-convergence
- **Laborat√≥rio**: implementa√ß√£o DCGAN completa
  - Training loop alternado
  - Visualiza√ß√£o de evolu√ß√£o
  - Experimentos com hiperpar√¢metros
  - Interpola√ß√£o no latent space

**Leituras**: Papers GAN (Goodfellow, 2014), DCGAN (Radford, 2015)

---

## **SEMANA 13 ‚Äî Diffusion Models e Estado da Arte**

### Estrutura da Aula (3 horas)
- **0:00‚Äì0:50**: Forward/Reverse Diffusion e Score Matching
- **0:50‚Äì1:00**: *Intervalo 1*
- **1:00‚Äì1:45**: DDPM: Training Objective e Implementa√ß√£o
- **1:45‚Äì1:55**: *Intervalo 2*
- **1:55‚Äì2:50**: Latent Diffusion Models (Stable Diffusion) e laborat√≥rio
- **2:50‚Äì3:00**: Fechamento

### Conte√∫do Detalhado

#### Bloco 1: Fundamentos de Diffusion (50 min)
- **Inspira√ß√£o**: termodin√¢mica n√£o-equilibrada
- **Forward process**: adi√ß√£o gradual de ru√≠do
  - q(x‚Çú | x‚Çú‚Çã‚ÇÅ) = N(x‚Çú; ‚àö(1-Œ≤‚Çú)x‚Çú‚Çã‚ÇÅ, Œ≤‚ÇúI)

- **Reparametriza√ß√£o direta**:
  - x‚Çú = ‚àö(·æ±‚Çú)x‚ÇÄ + ‚àö(1-·æ±‚Çú)Œµ

- **Reverse process**: p_Œ∏(x‚Çú‚Çã‚ÇÅ|x‚Çú)
- **Score matching**: conex√£o te√≥rica

#### Bloco 2: DDPM (45 min)
- **Loss simplificada**:
  - L_simple = ùîº[||Œµ - Œµ_Œ∏(x‚Çú, t)||¬≤]

- **Algoritmo de treinamento**: sample timestep aleat√≥rio, prever ru√≠do
- **Sampling algorithm**: denoising iterativo de x_T a x‚ÇÄ
- **Arquitetura U-Net**: time embedding, self-attention, ResNet blocks

#### Bloco 3: Latent Diffusion e Stable Diffusion (55 min)
- **Problema**: pixel space √© caro
- **Solu√ß√£o**: diffusion no latent space de VAE

- **Componentes Stable Diffusion**:
  1. VAE (Encoder/Decoder)
  2. U-Net (denoising no latent space)
  3. CLIP Text Encoder
  4. Cross-attention conditioning

- **Classifier-Free Guidance**:
  - ŒµÃÉ = Œµ_uncond + s¬∑(Œµ_cond - Œµ_uncond)

- **Laborat√≥rio**: experimenta√ß√£o com Stable Diffusion
  - Variar guidance scale
  - Negative prompts
  - N√∫mero de steps
  - Interpola√ß√£o entre prompts

**Leituras**: Papers DDPM (Ho, 2020), Stable Diffusion (Rombach, 2022)

---

## **SEMANA 14 ‚Äî Interpretabilidade, Robustez e √âtica**

### Estrutura da Aula (3 horas)
- **0:00‚Äì0:50**: Interpretabilidade e XAI (SHAP, LIME, Grad-CAM)
- **0:50‚Äì1:00**: *Intervalo 1*
- **1:00‚Äì1:45**: Robustez Adversarial: Ataques e Defesas
- **1:45‚Äì1:55**: *Intervalo 2*
- **1:55‚Äì2:50**: √âtica em IA: Vi√©s, Fairness, Privacidade
- **2:50‚Äì3:00**: Fechamento

### Conte√∫do Detalhado

#### Bloco 1: Interpretabilidade (50 min)
- **Problema da "caixa preta"**
- **Saliency Maps**: S = |‚àÇy_c/‚àÇx|
- **Grad-CAM**: heatmaps em CNNs
- **LIME**: explica√ß√£o local via modelo linear
- **SHAP**: Shapley values da teoria dos jogos

#### Bloco 2: Robustez Adversarial (45 min)
- **Exemplos adversariais**: x_adv = x + Œ¥
- **FGSM**: x_adv = x + Œµ¬∑sign(‚àá_x J)
- **PGD**: FGSM iterativo com proje√ß√£o
- **C&W**: otimiza√ß√£o sofisticada
- **Ataques f√≠sicos**: adversarial patches
- **Defesas**:
  - Adversarial training (mais efetiva)
  - Defensive distillation
  - Input transformations
  - Certified defenses

#### Bloco 3: √âtica em IA (55 min)
- **Vi√©s algor√≠tmico**:
  - Fontes: dataset, label, model, deployment bias
  - Casos reais: COMPAS, Amazon recruiting, facial recognition

- **Defini√ß√µes de Fairness**:
  - Demographic parity: P(≈∂=1|A=0) = P(≈∂=1|A=1)
  - Equal opportunity
  - Equalized odds

- **Privacidade**: differential privacy, federated learning
- **Uso respons√°vel**: deepfakes, environmental impact, automa√ß√£o
- **AI Safety**: alignment, specification gaming

- **Laborat√≥rio**: detectando e mitigando vi√©s

**Leituras**: Papers sobre Fairness, Adversarial Examples, AI Ethics

---

## **SEMANA 15 ‚Äî Apresenta√ß√µes e Perspectivas Futuras**

### Estrutura da Aula (3 horas)
- **0:00‚Äì1:00**: Apresenta√ß√µes de Projetos Finais (Parte 1)
- **1:00‚Äì1:10**: *Intervalo 1*
- **1:10‚Äì2:00**: Apresenta√ß√µes de Projetos Finais (Parte 2)
- **2:00‚Äì2:10**: *Intervalo 2*
- **2:10‚Äì2:50**: Fronteiras da Pesquisa e Perspectivas Futuras
- **2:50‚Äì3:00**: Encerramento

### Conte√∫do Detalhado

#### Blocos 1-2: Apresenta√ß√µes de Projetos (110 min total)
- **Formato**: 10 min apresenta√ß√£o + 2-3 min Q&A
- **8-10 projetos** no total

**Categorias de Projetos**:
- Reprodu√ß√£o de paper recente
- Aplica√ß√£o original
- Estudo comparativo
- Extens√£o te√≥rica

**Crit√©rios de Avalia√ß√£o**:
- Implementa√ß√£o t√©cnica (30%)
- Profundidade te√≥rica (25%)
- Qualidade experimental (25%)
- Apresenta√ß√£o (20%)

#### Bloco 3: Fronteiras da Pesquisa (40 min)

**T√≥picos Emergentes**:
1. **Modelos Multimodais**: CLIP, GPT-4V, Embodied AI
2. **Efficient AI**: quantization, pruning, distillation
3. **Neuro-Symbolic AI**: integra√ß√£o com racioc√≠nio simb√≥lico
4. **Graph Neural Networks**: dados n√£o-Euclidianos
5. **Continual Learning**: evitar catastrophic forgetting
6. **Foundation Models**: modelos multi-prop√≥sito massivos

**Desafios Abertos**:
- Reasoning complexo multi-step
- Sample efficiency
- Interpretabilidade profunda
- Robustez out-of-distribution
- AGI e alignment

**Carreira e Oportunidades**:
- Pesquisa acad√™mica vs. ind√∫stria
- ML Engineer, MLOps, Data Scientist
- Habilidades valorizadas
- Recursos para continuar aprendendo

#### Encerramento (10 min)
- Reflex√£o sobre a jornada (Semanas 1-15)
- Princ√≠pios para levar adiante
- Feedback da disciplina
- Agradecimentos e despedida

---

## **RESUMO DO PROGRAMA**

### Estrutura Modular
```
Semanas 1-3:  Fundamentos (Perceptron ‚Üí MLP ‚Üí Regulariza√ß√£o)
Semanas 4-5:  Vis√£o Computacional (CNNs Cl√°ssicas e Modernas)
Semanas 6-7:  Sequ√™ncias (RNNs, LSTMs, Seq2Seq)
Semanas 8-10: Revolu√ß√£o Transformer (Aten√ß√£o ‚Üí Transformers ‚Üí LLMs) ‚≠ê
Semanas 11-13: Modelos Generativos (VAE ‚Üí GAN ‚Üí Diffusion) ‚≠ê
Semana 14:    Responsabilidade (Interpretabilidade, Robustez, √âtica)
Semana 15:    Integra√ß√£o (Projetos e Perspectivas Futuras)
```

### Avalia√ß√£o
- **Projeto Final**: 40%
- **Listas de Exerc√≠cios**: 30% (3 listas)
- **Paper Review**: 15%
- **Participa√ß√£o**: 15%

### Bibliografia Principal
1. Goodfellow, I., Bengio, Y., & Courville, A. (2016). *Deep Learning*. MIT Press.
2. Chollet, F. (2021). *Deep Learning with Python* (2nd ed.). Manning.
3. Aggarwal, C. C. (2018). *Neural Networks and Deep Learning*. Springer.
4. Weidman, S. (2019). *Deep Learning from Scratch*. O'Reilly.

### Papers Fundamentais
- Vaswani et al. (2017): "Attention Is All You Need"
- Goodfellow et al. (2014): "Generative Adversarial Networks"
- Kingma & Welling (2013): "Auto-Encoding Variational Bayes"
- Ho et al. (2020): "Denoising Diffusion Probabilistic Models"
- Devlin et al. (2018): "BERT"
- Brown et al. (2020): "GPT-3"
- Rombach et al. (2022): "Stable Diffusion"

### Pr√©-requisitos
- √Ålgebra Linear
- C√°lculo Diferencial
- Probabilidade e Estat√≠stica
- Python (intermedi√°rio)
- Machine Learning (b√°sico)

---

**Elaborado para**: Programa de P√≥s-Gradua√ß√£o em Computa√ß√£o (Mestrado/Doutorado)  
**Vers√£o**: 2025.2  
**√öltima atualiza√ß√£o**: Outubro 2025